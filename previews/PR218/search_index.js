var documenterSearchIndex = {"docs":
[{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"L. Zhang, B. Carpenter, A. Gelman and A. Vehtari. Pathfinder: Parallel Quasi-Newton Variational Inference. Journal of Machine Learning Research 23, 1–49 (2022), arXiv:2108.03782. Accessed on Dec 6, 2024.\n\n\n\nR. H. Byrd, J. Nocedal and R. B. Schnabel. Representations of Quasi-Newton Matrices and Their Use in Limited Memory Methods. Mathematical Programming 63, 129–156 (1994).\n\n\n\nStan Reference Manual: HMC algorithm parameters. Accessed on Dec 6, 2024.\n\n\n\n","category":"page"},{"location":"lib/internals/#Internals","page":"Internals","title":"Internals","text":"","category":"section"},{"location":"lib/internals/","page":"Internals","title":"Internals","text":"Documentation for Pathfinder.jl's internal functions.","category":"page"},{"location":"lib/internals/","page":"Internals","title":"Internals","text":"See the Public Documentation section for documentation of the public interface.","category":"page"},{"location":"lib/internals/#Index","page":"Internals","title":"Index","text":"","category":"section"},{"location":"lib/internals/","page":"Internals","title":"Internals","text":"Pages = [\"internals.md\"]","category":"page"},{"location":"lib/internals/#Internal-Interface","page":"Internals","title":"Internal Interface","text":"","category":"section"},{"location":"lib/internals/","page":"Internals","title":"Internals","text":"Modules = [Pathfinder]\nPublic = false\nPrivate = true","category":"page"},{"location":"lib/internals/#Pathfinder.RankUpdateEuclideanMetric","page":"Internals","title":"Pathfinder.RankUpdateEuclideanMetric","text":"RankUpdateEuclideanMetric{T,M} <: AdvancedHMC.AbstractMetric\n\nA Gaussian Euclidean metric whose inverse is constructed by rank-updates.\n\nConstructors\n\nRankUpdateEuclideanMetric(n::Int)\nRankUpdateEuclideanMetric(M⁻¹::Pathfinder.WoodburyPDMat)\n\nConstruct a Gaussian Euclidean metric of size (n, n) with inverse of M⁻¹.\n\nExample\n\njulia> using LinearAlgebra, Pathfinder, AdvancedHMC\n\njulia> Pathfinder.RankUpdateEuclideanMetric(3)\nRankUpdateEuclideanMetric(diag=[1.0, 1.0, 1.0])\n\njulia> W = Pathfinder.WoodburyPDMat(Diagonal([0.1, 0.2]), [0.7 0.2]', Diagonal([0.3]))\n2×2 Pathfinder.WoodburyPDMat{Float64, Diagonal{Float64, Vector{Float64}}, Adjoint{Float64, Matrix{Float64}}, Diagonal{Float64, Vector{Float64}}, Diagonal{Float64, Vector{Float64}}, QRCompactWYQ{Float64, Matrix{Float64}, Matrix{Float64}}, UpperTriangular{Float64, Matrix{Float64}}}:\n 0.247  0.042\n 0.042  0.212\n\njulia> Pathfinder.RankUpdateEuclideanMetric(W)\nRankUpdateEuclideanMetric(diag=[0.247, 0.21200000000000002])\n\n\n\n\n\n","category":"type"},{"location":"lib/internals/#Pathfinder.UniformSampler","page":"Internals","title":"Pathfinder.UniformSampler","text":"UniformSampler(scale::Real)\n\nSampler that in-place modifies an array to be IID uniformly distributed on [-scale, scale]\n\n\n\n\n\n","category":"type"},{"location":"lib/internals/#Pathfinder.WoodburyPDFactorization","page":"Internals","title":"Pathfinder.WoodburyPDFactorization","text":"WoodburyPDFactorization{T,F} <: Factorization{T}\n\nA \"square root\" factorization of a positive definite Woodbury matrix.\n\nSee pdfactorize, WoodburyPDRightFactor, WoodburyPDMat.\n\n\n\n\n\n","category":"type"},{"location":"lib/internals/#Pathfinder.WoodburyPDMat","page":"Internals","title":"Pathfinder.WoodburyPDMat","text":"WoodburyPDMat <: PDMats.AbstractPDMat\n\nLazily represents a real positive definite (PD) matrix as an update to a full-rank PD matrix.\n\nWoodburyPDMat(A, B, D)\n\nConstructs the n times n PD matrix\n\nW = A + B D B^mathrmT\n\nwhere A is an n times n full rank positive definite matrix, D is an m times m symmetric matrix, and B is an n times m matrix. Note that B and D must be chosen such that W is positive definite; otherwise an error will be thrown during construction.\n\nUpon construction, WoodburyPDMat calls pdfactorize to construct a WoodburyPDFactorization, which is used in its overloads.\n\nSee pdfactorize, WoodburyPDFactorization\n\n\n\n\n\n","category":"type"},{"location":"lib/internals/#Pathfinder.WoodburyPDRightFactor","page":"Internals","title":"Pathfinder.WoodburyPDRightFactor","text":"WoodburyPDRightFactor{T,TA,Q,TC} <: AbstractMatrix{T}\n\nThe right factor R of a WoodburyPDFactorization.\n\n\n\n\n\n","category":"type"},{"location":"lib/internals/#Pathfinder.fit_mvnormals-Tuple{Any, Any, Any}","page":"Internals","title":"Pathfinder.fit_mvnormals","text":"fit_mvnormals(θs, ∇logpθs; history_length=5) -> (dists, num_bfgs_updates_rejected)\n\nFit a multivariate-normal distribution to each point on the trajectory θs.\n\nGiven θs with gradients ∇logpθs, construct LBFGS inverse Hessian approximations with the provided history_length. The inverse Hessians approximate a covariance. The covariances and corresponding means that define multivariate normal approximations per point are returned.\n\nThe 2nd returned value is the number of BFGS updates to the inverse Hessian matrices that were rejected due to keeping the inverse Hessian positive definite.\n\n\n\n\n\n","category":"method"},{"location":"lib/internals/#Pathfinder.lbfgs_inverse_hessian!-Tuple{Pathfinder.LBFGSInverseHessianCache, Pathfinder.LBFGSHistory}","page":"Internals","title":"Pathfinder.lbfgs_inverse_hessian!","text":"lbfgs_inverse_hessian!(cache::LBFGSInverseHessianCache, history::LBFGSHistory) -> WoodburyPDMat\n\nCompute approximate inverse Hessian initialized from history stored in cache and history.\n\ncache stores the diagonal of the initial inverse Hessian H₀^-1 and the matrices B₀ and D₀, which are overwritten here and are used in the construction of the returned approximate inverse Hessian H^-1.\n\nFrom Byrd et al. [2], Theorem 2.2, the expression for the inverse Hessian H^-1 is\n\nbeginalign\nB = beginpmatrixH_0^-1 Y  Sendpmatrix\nR = operatornametriu(S^mathrmT Y)\nE = I circ R\nD = beginpmatrix\n    0  -R^-1\n    -R^-mathrmT  R^mathrm-T (E + Y^mathrmT H_0 Y ) R^mathrm-1\nendpmatrix\nH^-1 = H_0^-1 + B D B^mathrmT\nendalign\n\nReferences\n\n[2]: Byrd et al. Math. Program. 63, 1994.\n\n\n\n\n\n","category":"method"},{"location":"lib/internals/#Pathfinder.lbfgs_inverse_hessians-Tuple{Any, Any, Any}","page":"Internals","title":"Pathfinder.lbfgs_inverse_hessians","text":"lbfgs_inverse_hessians(\n    θs, ∇logpθs; Hinit=gilbert_init, history_length=5, ϵ=1e-12\n) -> Tuple{Vector{WoodburyPDMat},Int}\n\nFrom an L-BFGS trajectory and gradients, compute the inverse Hessian approximations at each point.\n\nGiven positions θs with gradients ∇logpθs, construct LBFGS inverse Hessian approximations with the provided history_length.\n\nThe 2nd returned value is the number of BFGS updates to the inverse Hessian matrices that were rejected due to keeping the inverse Hessian positive definite.\n\n\n\n\n\n","category":"method"},{"location":"lib/internals/#Pathfinder.pdfactorize-Tuple{AbstractMatrix, AbstractMatrix, AbstractMatrix}","page":"Internals","title":"Pathfinder.pdfactorize","text":"pdfactorize(A, B, D) -> WoodburyPDFactorization\n\nFactorize the positive definite matrix W = A + B D B^mathrmT.\n\nThe result is the \"square root\" factorization (L, R), where W = L R and L = R^mathrmT.\n\nLet U^mathrmT U = A be the Cholesky decomposition of A, and let Q X = U^-mathrmT B be a thin QR decomposition. Define C = I + XDX^mathrmT, with the Cholesky decomposition V^mathrmT V = C. Then, W = R^mathrmT R, where\n\nR = beginpmatrix U  0  0  I endpmatrix Q^mathrmT V\n\nThe positive definite requirement is equivalent to the requirement that both A and C are positive definite.\n\nFor a derivation of this decomposition for the special case of diagonal A, see Zhang et al. [1], appendix A.\n\nSee pdunfactorize, WoodburyPDFactorization, WoodburyPDMat\n\nReferences\n\n[1]: Zhang et al. JMLR 23(306), 2022.\n\n\n\n\n\n","category":"method"},{"location":"lib/internals/#Pathfinder.pdunfactorize-Tuple{Pathfinder.WoodburyPDFactorization}","page":"Internals","title":"Pathfinder.pdunfactorize","text":"pdunfactorize(F::WoodburyPDFactorization) -> (A, B, D)\n\nPerform a reverse operation to pdfactorize.\n\nNote that this function does not compute the inverse of pdfactorize. It only computes matrices that produce the same matrix W = A + B D B^mathrmT as for the inputs to pdfactorize.\n\n\n\n\n\n","category":"method"},{"location":"lib/internals/#Pathfinder.resample-NTuple{4, Any}","page":"Internals","title":"Pathfinder.resample","text":"resample(rng, x, log_weights, ndraws) -> (draws, psis_result)\nresample(rng, x, ndraws) -> draws\n\nDraw ndraws samples from x, with replacement.\n\nIf log_weights is provided, perform Pareto smoothed importance resampling.\n\n\n\n\n\n","category":"method"},{"location":"examples/quickstart/#Quickstart","page":"Quickstart","title":"Quickstart","text":"","category":"section"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"This page introduces basic Pathfinder usage with examples.","category":"page"},{"location":"examples/quickstart/#A-5-dimensional-multivariate-normal","page":"Quickstart","title":"A 5-dimensional multivariate normal","text":"","category":"section"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"For a simple example, we'll run Pathfinder on a multivariate normal distribution with a dense covariance matrix. Pathfinder can take a log-density function. By default, the gradient of the log-density function is computed using ForwardDiff.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"using ADTypes, ForwardDiff, LinearAlgebra, LogDensityProblems,\n      Pathfinder, Printf, ReverseDiff, StatsPlots, Random\nRandom.seed!(42)\n\nstruct MvNormalProblem{T,S}\n    μ::T  # mean\n    P::S  # precision matrix\nend\nfunction (prob::MvNormalProblem)(x)\n    z = x - prob.μ\n    return -dot(z, prob.P, z) / 2\nend\n\nΣ = [\n    2.71   0.5    0.19   0.07   1.04\n    0.5    1.11  -0.08  -0.17  -0.08\n    0.19  -0.08   0.26   0.07  -0.7\n    0.07  -0.17   0.07   0.11  -0.21\n    1.04  -0.08  -0.7   -0.21   8.65\n]\nμ = [-0.55, 0.49, -0.76, 0.25, 0.94]\nP = inv(Symmetric(Σ))\nprob_mvnormal = MvNormalProblem(μ, P)\n\nnothing # hide","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"Now we run pathfinder.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"result = pathfinder(prob_mvnormal; dim=5, init_scale=4)","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"result is a PathfinderResult. See its docstring for a description of its fields.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"The L-BFGS optimizer constructs an approximation to the inverse Hessian of the negative log density using the limited history of previous points and gradients. For each iteration, Pathfinder uses this estimate as an approximation to the covariance matrix of a multivariate normal that approximates the target distribution. The distribution that maximizes the evidence lower bound (ELBO) is stored in result.fit_distribution. Its mean and covariance are quite close to our target distribution's.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"result.fit_distribution.μ","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"result.fit_distribution.Σ","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"result.draws is a Matrix whose columns are the requested draws from result.fit_distribution:","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"result.draws","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"We can visualize Pathfinder's sequence of multivariate-normal approximations with the following function:","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"function plot_pathfinder_trace(\n    result, logp_marginal, xrange, yrange, maxiters;\n    show_elbo=false, flipxy=false, kwargs...,\n)\n    iterations = min(length(result.optim_trace) - 1, maxiters)\n    trace_points = result.optim_trace.points\n    trace_dists = result.fit_distributions\n    anim = @animate for i in 1:iterations\n        contour(xrange, yrange, exp ∘ logp_marginal ∘ Base.vect; label=\"\")\n        trace = trace_points[1:(i + 1)]\n        dist = trace_dists[i + 1]\n        plot!(\n            first.(trace), getindex.(trace, 2);\n            seriestype=:scatterpath, color=:black, msw=0, label=\"trace\",\n        )\n        covellipse!(\n            dist.μ[1:2], dist.Σ[1:2, 1:2];\n            n_std=2.45, alpha=0.7, color=1, linecolor=1, label=\"MvNormal 95% ellipsoid\",\n        )\n        title = \"Iteration $i\"\n        if show_elbo\n            est = result.elbo_estimates[i]\n            title *= \"  ELBO estimate: \" * @sprintf(\"%.1f\", est.value)\n        end\n        plot!(; xlims=extrema(xrange), ylims=extrema(yrange), title, kwargs...)\n    end\n    return anim\nend;\nnothing #hide","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"xrange = -5:0.1:5\nyrange = -5:0.1:5\n\nμ_marginal = μ[1:2]\nP_marginal = inv(Σ[1:2,1:2])\nlogp_mvnormal_marginal(x) = -dot(x - μ_marginal, P_marginal, x - μ_marginal) / 2\n\nanim = plot_pathfinder_trace(\n    result, logp_mvnormal_marginal, xrange, yrange, 20;\n    xlabel=\"x₁\", ylabel=\"x₂\",\n)\ngif(anim; fps=5)","category":"page"},{"location":"examples/quickstart/#A-banana-shaped-distribution","page":"Quickstart","title":"A banana-shaped distribution","text":"","category":"section"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"Now we will run Pathfinder on the following banana-shaped distribution with density","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"pi(x_1 x_2) = e^-x_1^2  2 e^-5 (x_2 - x_1^2)^2  2","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"Pathfinder can also take any object that implements the LogDensityProblems interface interface. This can also be used to manually define the gradient of the log-density function.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"First we define the log density problem:","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"Random.seed!(23)\n\nstruct BananaProblem end\nfunction LogDensityProblems.capabilities(::Type{<:BananaProblem})\n    return LogDensityProblems.LogDensityOrder{1}()\nend\nLogDensityProblems.dimension(::BananaProblem) = 2\nfunction LogDensityProblems.logdensity(::BananaProblem, x)\n    return -(x[1]^2 + 5(x[2] - x[1]^2)^2) / 2\nend\nfunction LogDensityProblems.logdensity_and_gradient(::BananaProblem, x)\n    a = (x[2] - x[1]^2)\n    lp = -(x[1]^2 + 5a^2) / 2\n    grad_lp = [(10a - 1) * x[1], -5a]\n    return lp, grad_lp\nend\n\nprob_banana = BananaProblem()\n\nnothing # hide","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"and then visualise it:","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"xrange = -3.5:0.05:3.5\nyrange = -3:0.05:7\nlogp_banana(x) = LogDensityProblems.logdensity(prob_banana, x)\ncontour(xrange, yrange, exp ∘ logp_banana ∘ Base.vect; xlabel=\"x₁\", ylabel=\"x₂\")","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"Now we run pathfinder.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"result = pathfinder(prob_banana; init_scale=10)","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"As before we can visualise each iteration of the algorithm.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"anim = plot_pathfinder_trace(\n    result, logp_banana, xrange, yrange, 20;\n    xlabel=\"x₁\", ylabel=\"x₂\",\n)\ngif(anim; fps=5)","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"Since the distribution is far from normal, Pathfinder is unable to fit the distribution well. Especially for such complicated target distributions, it's always a good idea to run multipathfinder, which runs single-path Pathfinder multiple times.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"ndraws = 1_000\nresult = multipathfinder(prob_banana, ndraws; nruns=20, init_scale=10)","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"result is a MultiPathfinderResult. See its docstring for a description of its fields.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"result.fit_distribution is a uniformly-weighted Distributions.MixtureModel. Each component is the result of a single Pathfinder run. It's possible that some runs fit the target distribution much better than others, so instead of just drawing samples from result.fit_distribution, multipathfinder draws many samples from each component and then uses Pareto-smoothed importance resampling (using PSIS.jl) from these draws to better target prob_banana.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"The Pareto shape diagnostic informs us on the quality of these draws. Here the Pareto shape k diagnostic is bad (k  07), which tells us that these draws are unsuitable for computing posterior estimates, so we should definitely run MCMC to get better draws. Still, visualizing the draws can still be useful.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"x₁_approx = result.draws[1, :]\nx₂_approx = result.draws[2, :]\n\ncontour(xrange, yrange, exp ∘ logp_banana ∘ Base.vect)\nscatter!(x₁_approx, x₂_approx; msw=0, ms=2, alpha=0.5, color=1)\nplot!(xlims=extrema(xrange), ylims=extrema(yrange), xlabel=\"x₁\", ylabel=\"x₂\", legend=false)","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"While the draws do a poor job of covering the tails of the distribution, they are still useful for identifying the nonlinear correlation between these two parameters.","category":"page"},{"location":"examples/quickstart/#A-100-dimensional-funnel","page":"Quickstart","title":"A 100-dimensional funnel","text":"","category":"section"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"As we have seen above, running multi-path Pathfinder is much more useful for target distributions that are far from normal. One particularly difficult distribution to sample is Neal's funnel:","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"beginaligned\ntau sim mathrmNormal(mu=0 sigma=3)\nbeta_i sim mathrmNormal(mu=0 sigma=e^tau2)\nendaligned","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"Such funnel geometries appear in other models (e.g. many hierarchical models) and typically frustrate MCMC sampling. Multi-path Pathfinder can't sample the funnel well, but it can quickly give us draws that can help us diagnose that we have a funnel.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"In this example, we draw from a 100-dimensional funnel and visualize 2 dimensions.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"using ReverseDiff, ADTypes\n\nRandom.seed!(68)\n\nfunction logp_funnel(x)\n    n = length(x)\n    τ = x[1]\n    β = view(x, 2:n)\n    return ((τ / 3)^2 + (n - 1) * τ + sum(b -> abs2(b * exp(-τ / 2)), β)) / -2\nend\n\nnothing # hide","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"First, let's fit this posterior with single-path Pathfinder. For high-dimensional problems, it's better to use reverse-mode automatic differentiation. Here, we'll use ADTypes.AutoReverseDiff to specify that ReverseDiff.jl should be used.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"result_single = pathfinder(logp_funnel; dim=100, init_scale=10, adtype=AutoReverseDiff())","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"Let's visualize this sequence of multivariate normals for the first two dimensions.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"β₁_range = -5:0.01:5\nτ_range = -15:0.01:5\n\nanim = plot_pathfinder_trace(\n    result_single, logp_funnel, τ_range, β₁_range, 15;\n    show_elbo=true, xlabel=\"τ\", ylabel=\"β₁\",\n)\ngif(anim; fps=2)","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"For this challenging posterior, we can again see that most of the approximations are not great, because this distribution is not normal. Also, this distribution has a pole instead of a mode, so there is no MAP estimate, and no Laplace approximation exists. As optimization proceeds, the approximation goes from very bad to less bad and finally extremely bad. The ELBO-maximizing distribution is at the neck of the funnel, which would be a good location to initialize MCMC.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"Now we run multipathfinder.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"ndraws = 1_000\nresult = multipathfinder(logp_funnel, ndraws; dim=100, nruns=20, init_scale=10, adtype=AutoReverseDiff())","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"Again, the poor Pareto shape diagnostic indicates we should run MCMC to get draws suitable for computing posterior estimates.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"We can see that the bulk of Pathfinder's draws come from the neck of the funnel, where the fit from the single path we examined was located.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"τ_approx = result.draws[1, :]\nβ₁_approx = result.draws[2, :]\n\ncontour(τ_range, β₁_range, exp ∘ logp_funnel ∘ Base.vect)\nscatter!(τ_approx, β₁_approx; msw=0, ms=2, alpha=0.5, color=1)\nplot!(; xlims=extrema(τ_range), ylims=extrema(β₁_range), xlabel=\"τ\", ylabel=\"β₁\", legend=false)","category":"page"},{"location":"examples/turing/#Running-Pathfinder-on-Turing.jl-models","page":"Turing usage","title":"Running Pathfinder on Turing.jl models","text":"","category":"section"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"This tutorial demonstrates how Turing can be used with Pathfinder.","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"We'll demonstrate with a regression example.","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"using AdvancedHMC, Pathfinder, Random, Turing\nRandom.seed!(39)\n\n@model function regress(x, y)\n    α ~ Normal()\n    β ~ Normal()\n    σ ~ truncated(Normal(); lower=0)\n    y .~ Normal.(α .+ β .* x, σ)\nend\nx = 0:0.1:10\ny = @. 2x + 1.5 + randn() * 0.2\nnothing # hide","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"model = regress(collect(x), y)\nn_chains = 8","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"For convenience, pathfinder and multipathfinder can take Turing models as inputs and produce MCMCChains.Chains objects as outputs.","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"result_single = pathfinder(model; ndraws=1_000)","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"result_multi = multipathfinder(model, 1_000; nruns=n_chains)","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"Here, the Pareto shape diagnostic indicates that it is likely safe to use these draws to compute posterior estimates.","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"When passed a DynamicPPL.Model, Pathfinder also gives access to the posterior draws in a familiar Chains object.","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"result_multi.draws_transformed","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"We can also use these posterior draws to initialize MCMC sampling.","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"init_params = collect.(eachrow(result_multi.draws_transformed.value[1:n_chains, :, 1]))","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"chns = sample(model, Turing.NUTS(), MCMCThreads(), 1_000, n_chains; init_params, progress=false)","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"We can use Pathfinder's estimate of the metric and only perform enough warm-up to tune the step size.","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"inv_metric = result_multi.pathfinder_results[1].fit_distribution.Σ\nmetric = Pathfinder.RankUpdateEuclideanMetric(inv_metric)\nkernel = HMCKernel(Trajectory{MultinomialTS}(Leapfrog(0.0), GeneralisedNoUTurn()))\nadaptor = StepSizeAdaptor(0.8, 1.0)  # adapt only the step size\nnuts = AdvancedHMC.HMCSampler(kernel, metric, adaptor)\n\nn_adapts = 50\nn_draws = 1_000\nchns = sample(\n    model,\n    externalsampler(nuts),\n    MCMCThreads(),\n    n_draws + n_adapts,\n    n_chains;\n    n_adapts,\n    init_params,\n    progress=false,\n)[n_adapts + 1:end, :, :]  # drop warm-up draws","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"See Initializing HMC with Pathfinder for further examples.","category":"page"},{"location":"lib/public/#Public-Documentation","page":"Public","title":"Public Documentation","text":"","category":"section"},{"location":"lib/public/","page":"Public","title":"Public","text":"Documentation for Pathfinder.jl's public interface.","category":"page"},{"location":"lib/public/","page":"Public","title":"Public","text":"See the Internals section for documentation of internal functions.","category":"page"},{"location":"lib/public/#Index","page":"Public","title":"Index","text":"","category":"section"},{"location":"lib/public/","page":"Public","title":"Public","text":"Pages = [\"public.md\"]","category":"page"},{"location":"lib/public/#Public-Interface","page":"Public","title":"Public Interface","text":"","category":"section"},{"location":"lib/public/","page":"Public","title":"Public","text":"Modules = [Pathfinder]\nPages = [\"singlepath.jl\"]\nOrder = [:function, :type]\nPublic = true\nPrivate = false","category":"page"},{"location":"lib/public/#Pathfinder.pathfinder","page":"Public","title":"Pathfinder.pathfinder","text":"pathfinder(fun; kwargs...)\n\nFind the best multivariate normal approximation encountered while maximizing a log density.\n\nFrom an optimization trajectory, Pathfinder constructs a sequence of (multivariate normal) approximations to the distribution specified by a log density function. The approximation that maximizes the evidence lower bound (ELBO), or equivalently, minimizes the KL divergence between the approximation and the true distribution, is returned.\n\nThe covariance of the multivariate normal distribution is an inverse Hessian approximation constructed using at most the previous history_length steps.\n\nArguments\n\nfun: An object representing the log-density of the target distribution. Supported   types include:\na callable with the signature   f(params::AbstractVector{<:Real}) -> log_density::Real.\nan object implementing the   LogDensityProblems interface.\nSciMLBase.OptimizationFunction: wraps the negative log density. It must   have the necessary features (e.g. a gradient or Hessian function) for the chosen   optimizer.\nSciMLBase.OptimizationProblem: an optimization problem containing a   function with the same properties as the above OptimizationFunction, as well as an   initial point. If provided, init and dim are ignored.\nDynamicPPL.Model: a Turing model. If provided, init and dim are   ignored.\n\nKeywords\n\ndim::Int: dimension of the target distribution. Ignored if init provided.\ninit::AbstractVector{<:Real}: initial point of length dim from which to begin   optimization. If not provided and fun does not contain an initial point, an initial   point of type Vector{Float64} and length dim is created and filled using   init_sampler.\ninit_scale::Real: scale factor s such that the default init_sampler samples   entries uniformly in the range -s s\ninit_sampler: function with the signature (rng, x) -> x that modifies a vector of   length dims in-place to generate an initial point\nndraws_elbo::Int=5: Number of draws used to estimate the ELBO\nndraws::Int=ndraws_elbo: number of approximate draws to return\nrng::Random.AbstractRNG: The random number generator to be used for drawing samples\nexecutor::Transducers.Executor: Transducers.jl executor that   determines if and how to perform ELBO computation in parallel. The default   (Transducers.SequentialEx()) performs no   parallelization. If rng is known to be thread-safe, and the log-density function is   known to have no internal state, then   Transducers.PreferParallel() may be used to   parallelize log-density evaluation. This is generally only faster for expensive log   density functions.\nhistory_length::Int=6: Size of the history used to approximate the   inverse Hessian.\noptimizer: Optimizer to be used for constructing trajectory. Can be any optimizer   compatible with Optimization.jl, so long   as it supports callbacks. Defaults to   Optim.LBFGS(; m=history_length, linesearch=LineSearches.HagerZhang(), alphaguess=LineSearches.InitialHagerZhang()).\nsave_trace::Bool=true: Whether to save the optimization trace and intermediate distributions.   If false, fit_distributions will be empty.\nadtype::ADTypes.AbstractADType: Specifies which automatic   differentiation backend should be used to compute the gradient, if fun does not   already specify the gradient. Default is   ADTypes.AutoForwardDiff() See   Optimization.jl's Automatic Differentiation Recommendations.\nntries::Int=1_000: Number of times to try the optimization, restarting if it fails.   Before every restart, a new initial point is drawn using init_sampler.\nfail_on_nonfinite::Bool=true: If true, optimization fails if the log-density is a   NaN or Inf or if the gradient is ever non-finite. If nretries > 0, then   optimization will be retried after reinitialization.\nkwargs... : Remaining keywords are forwarded to   Optimization.solve.\n\nReturns\n\nPathfinderResult\n\n\n\n\n\n","category":"function"},{"location":"lib/public/#Pathfinder.PathfinderResult","page":"Public","title":"Pathfinder.PathfinderResult","text":"PathfinderResult\n\nContainer for results of single-path Pathfinder.\n\nFields\n\ninput: User-provided input object, e.g. a LogDensityProblem, optim_fun, optim_prob,   or another object.\noptimizer: Optimizer used for maximizing the log-density\nrng: Pseudorandom number generator that was used for sampling\noptim_prob::SciMLBase.OptimizationProblem: Optimization problem used for   optimization\nlogp: Log-density function\nfit_distribution::Distributions.MvNormal: ELBO-maximizing multivariate   normal distribution\ndraws::AbstractMatrix{<:Real}: draws from multivariate normal with size (dim, ndraws)\nfit_distribution_transformed: fit_distribution transformed to the same space as the   user-supplied target distribution. This is only different from fit_distribution when   integrating with other packages, and its type depends on the type of input.\ndraws_transformed: draws transformed to be draws from fit_distribution_transformed.\nfit_iteration::Int: Iteration at which ELBO estimate was maximized\nnum_tries::Int: Number of tries until Pathfinder succeeded\noptim_solution::SciMLBase.OptimizationSolution: Solution object of   optimization.\noptim_trace::Pathfinder.OptimizationTrace: container for optimization trace of points,   log-density, and gradient. The first point is the initial point.\nfit_distributions::AbstractVector{Distributions.MvNormal}: Multivariate normal   distributions for each point in optim_trace, where   fit_distributions[fit_iteration + 1] == fit_distribution\nelbo_estimates::AbstractVector{<:Pathfinder.ELBOEstimate}: ELBO estimates for all but   the first distribution in fit_distributions.\nnum_bfgs_updates_rejected::Int: Number of times a BFGS update to the reconstructed   inverse Hessian was rejected to keep the inverse Hessian positive definite.\n\nReturns\n\nPathfinderResult\n\n\n\n\n\n","category":"type"},{"location":"lib/public/","page":"Public","title":"Public","text":"Modules = [Pathfinder]\nPages = [\"multipath.jl\"]\nOrder = [:function, :type]\nPublic = true\nPrivate = false","category":"page"},{"location":"lib/public/#Pathfinder.multipathfinder","page":"Public","title":"Pathfinder.multipathfinder","text":"multipathfinder(fun, ndraws; kwargs...)\n\nRun pathfinder multiple times to fit a multivariate normal mixture model.\n\nFor nruns=length(init), nruns parallel runs of pathfinder produce nruns multivariate normal approximations q_k = q(phi  mu_k Sigma_k) of the posterior. These are combined to a mixture model q with uniform weights.\n\nq is augmented with the component index to generate random samples, that is, elements (k phi) are drawn from the augmented mixture model\n\ntildeq(phi k  mu Sigma) = K^-1 q(phi  mu_k Sigma_k)\n\nwhere k is a component index, and K= nruns. These draws are then resampled with replacement. Discarding k from the samples would reproduce draws from q.\n\nIf importance=true, then Pareto smoothed importance resampling is used, so that the resulting draws better approximate draws from the target distribution p instead of q. This also prints a warning message if the importance weighted draws are unsuitable for approximating expectations with respect to p.\n\nArguments\n\nfun: An object representing the log-density of the target distribution. Supported   types include:\na callable with the signature   f(params::AbstractVector{<:Real}) -> log_density::Real.\nan object implementing the   LogDensityProblems interface.\nSciMLBase.OptimizationFunction: wraps the negative log density. It must   have the necessary features (e.g. a gradient or Hessian function) for the chosen   optimizer.\nSciMLBase.OptimizationProblem: an optimization problem containing a   function with the same properties as the above OptimizationFunction, as well as an   initial point. If provided, init and dim are ignored.\nDynamicPPL.Model: a Turing model. If provided, init and dim are   ignored.\nndraws::Int: number of approximate draws to return\n\nKeywords\n\ninit: iterator of length nruns of initial points of length dim from which each   single-path Pathfinder run will begin. length(init) must be implemented. If init is   not provided, nruns must be, and dim must be if fun provided.\nnruns::Int: number of runs of Pathfinder to perform. Ignored if init is provided.\nndraws_per_run::Int: The number of draws to take for each component before resampling.   Defaults to a number such that ndraws_per_run * nruns > ndraws.\nimportance::Bool=true: Perform Pareto smoothed importance resampling of draws.\nrng::AbstractRNG=Random.GLOBAL_RNG: Pseudorandom number generator. It is recommended to   use a parallelization-friendly PRNG like the default PRNG on Julia 1.7 and up.\nexecutor::Transducers.Executor: Transducers.jl executor that determines if and how to   run the single-path runs in parallel, defaulting to   Transducers.SequentialEx(). If a transducer for   multi-threaded computation is selected, you must first verify that rng and the log   density function are thread-safe.\nexecutor_per_run::Transducers.Executor: Transducers.jl executor used within each run to   parallelize PRNG calls, defaulting to   Transducers.SequentialEx(). See   pathfinder for further description.\nkwargs... : Remaining keywords are forwarded to pathfinder.\n\nReturns\n\nMultiPathfinderResult\n\n\n\n\n\n","category":"function"},{"location":"lib/public/#Pathfinder.MultiPathfinderResult","page":"Public","title":"Pathfinder.MultiPathfinderResult","text":"MultiPathfinderResult\n\nContainer for results of multi-path Pathfinder.\n\nFields\n\ninput: User-provided input object, e.g. either logp, (logp, ∇logp), optim_fun,   optim_prob, or another object.\noptimizer: Optimizer used for maximizing the log-density\nrng: Pseudorandom number generator that was used for sampling\noptim_prob::SciMLBase.OptimizationProblem: Otimization problem used for   optimization\nlogp: Log-density function\nfit_distribution::Distributions.MixtureModel: uniformly-weighted mixture of   ELBO-maximizing multivariate normal distributions from each run.\ndraws::AbstractMatrix{<:Real}: draws from fit_distribution with size (dim, ndraws),   potentially resampled using importance resampling to be closer to the target   distribution.\ndraw_component_ids::Vector{Int}: component id of each draw in draws.\nfit_distribution_transformed: fit_distribution transformed to the same space as the   user-supplied target distribution. This is only different from fit_distribution when   integrating with other packages, and its type depends on the type of input.\ndraws_transformed: draws transformed to be draws from fit_distribution_transformed.\npathfinder_results::Vector{<:PathfinderResult}: results of each single-path   Pathfinder run.\npsis_result::Union{Nothing,<:PSIS.PSISResult}: If importance resampling   was used, the result of Pareto-smoothed importance resampling.   psis_result.pareto_shape also diagnoses whether draws can be used to compute   estimates from the target distribution.\n\n\n\n\n\n","category":"type"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = Pathfinder","category":"page"},{"location":"#Pathfinder.jl:-Parallel-quasi-Newton-variational-inference","page":"Home","title":"Pathfinder.jl: Parallel quasi-Newton variational inference","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package implements Pathfinder [1], a variational method for initializing Markov chain Monte Carlo (MCMC) methods.","category":"page"},{"location":"#Single-path-Pathfinder","page":"Home","title":"Single-path Pathfinder","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Single-path Pathfinder (pathfinder) attempts to return draws in or near the typical set, usually with many fewer gradient evaluations. Pathfinder uses the limited-memory BFGS(L-BFGS) optimizer to construct a maximum a posteriori (MAP) estimate of a target posterior distribution p. It then uses the trace of the optimization to construct a sequence of multivariate normal approximations to the target distribution, returning the approximation that maximizes the evidence lower bound (ELBO) – equivalently, minimizes the Kullback-Leibler divergence from the target distribution – as well as draws from the distribution.","category":"page"},{"location":"#Multi-path-Pathfinder","page":"Home","title":"Multi-path Pathfinder","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Multi-path Pathfinder (multipathfinder) consists of running Pathfinder multiple times. It returns a uniformly-weighted mixture model of the multivariate normal approximations of the individual runs. It also uses importance resampling to return samples that better approximate the target distribution and assess the quality of the approximation.","category":"page"},{"location":"#Uses","page":"Home","title":"Uses","text":"","category":"section"},{"location":"#Using-the-Pathfinder-draws","page":"Home","title":"Using the Pathfinder draws","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"note: Folk theorem of statistical computing\nWhen you have computational problems, often there’s a problem with your model.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Visualizing posterior draws is a common way to diagnose problems with a model.  However, problematic models often tend to be slow to warm-up. Even if the draws returned by Pathfinder are only approximations to the posterior, they can sometimes still be used to diagnose basic issues such as highly correlated parameters, parameters with very different posterior variances, and multimodality.","category":"page"},{"location":"#Initializing-MCMC","page":"Home","title":"Initializing MCMC","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pathfinder can be used to initialize MCMC. This especially useful when sampling with Hamiltonian Monte Carlo. See Initializing HMC with Pathfinder for details.","category":"page"},{"location":"#Integration-with-the-Julia-ecosystem","page":"Home","title":"Integration with the Julia ecosystem","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pathfinder uses several packages for extended functionality:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Optimization.jl: This allows the L-BFGS optimizer to be replaced with any of the many Optimization-compatible optimizers and supports use of callbacks. Note that any changes made to Pathfinder using these features would be experimental.\nADTypes.jl: Supports specifying the automatic differentiation engine to be used for computing gradient and Hessian, if needed.\nTransducers.jl: parallelization support\nDistributions.jl/PDMats.jl: fits can be used anywhere a Distribution can be used\nLogDensityProblems.jl: defining the log-density function, gradient, and Hessian\nProgressLogging.jl: In Pluto, Juno, and VSCode, nested progress bars are shown. In the REPL, use TerminalLoggers.jl to get progress bars.","category":"page"},{"location":"examples/initializing-hmc/#Initializing-HMC-with-Pathfinder","page":"Initializing HMC","title":"Initializing HMC with Pathfinder","text":"","category":"section"},{"location":"examples/initializing-hmc/#The-MCMC-warm-up-phase","page":"Initializing HMC","title":"The MCMC warm-up phase","text":"","category":"section"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"When using MCMC to draw samples from some target distribution, there is often a lengthy warm-up phase with 2 phases:","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"converge to the typical set (the region of the target distribution where the bulk of the probability mass is located)\nadapt any tunable parameters of the MCMC sampler (optional)","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"While (1) often happens fairly quickly, (2) usually requires a lengthy exploration of the typical set to iteratively adapt parameters suitable for further exploration. An example is the widely used windowed adaptation scheme of Hamiltonian Monte Carlo (HMC) in Stan [3], where a step size and positive definite metric (aka mass matrix) are adapted. For posteriors with complex geometry, the adaptation phase can require many evaluations of the gradient of the log density function of the target distribution.","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"Pathfinder can be used to initialize MCMC, and in particular HMC, in 3 ways:","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"Initialize MCMC from one of Pathfinder's draws (replace phase 1 of the warm-up).\nInitialize an HMC metric adaptation from the inverse of the covariance of the multivariate normal approximation (replace the first window of phase 2 of the warm-up).\nUse the inverse of the covariance as the metric without further adaptation (replace phase 2 of the warm-up).","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"This tutorial demonstrates all three approaches with DynamicHMC.jl and AdvancedHMC.jl. Both of these packages have standalone implementations of adaptive HMC (aka NUTS) and can be used independently of any probabilistic programming language (PPL). Both the Turing and Soss PPLs have some DynamicHMC integration, while Turing also integrates with AdvancedHMC.","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"For demonstration purposes, we'll use the following dummy data:","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"using LinearAlgebra, Pathfinder, Random, StatsFuns, StatsPlots\n\nRandom.seed!(91)\n\nx = 0:0.01:1\ny = @. sin(10x) + randn() * 0.2 + x\n\nscatter(x, y; xlabel=\"x\", ylabel=\"y\", legend=false, msw=0, ms=2)","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"We'll fit this using a simple polynomial regression model:","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"beginaligned\nsigma sim textHalf-Normal(mu=0 sigma=1)\nalpha beta_j sim mathrmNormal(mu=0 sigma=1)\nhaty_i = alpha + sum_j=1^J x_i^j beta_j\ny_i sim mathrmNormal(mu=haty_i sigma=sigma)\nendaligned","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"We just need to implement the log-density function of the resulting posterior.","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"struct RegressionProblem{X,Z,Y}\n    x::X\n    J::Int\n    z::Z\n    y::Y\nend\nfunction RegressionProblem(x, J, y)\n    z = x .* (1:J)'\n    return RegressionProblem(x, J, z, y)\nend\n\nfunction (prob::RegressionProblem)(θ)\n    σ = θ.σ\n    α = θ.α\n    β = θ.β\n    z = prob.z\n    y = prob.y\n    lp = normlogpdf(σ) + logtwo\n    lp += normlogpdf(α)\n    lp += sum(normlogpdf, β)\n    y_hat = muladd(z, β, α)\n    lp += sum(eachindex(y_hat, y)) do i\n        return normlogpdf(y_hat[i], σ, y[i])\n    end\n    return lp\nend\n\nJ = 3\ndim = J + 2\nmodel = RegressionProblem(x, J, y)\nndraws = 1_000;\nnothing # hide","category":"page"},{"location":"examples/initializing-hmc/#DynamicHMC.jl","page":"Initializing HMC","title":"DynamicHMC.jl","text":"","category":"section"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"To use DynamicHMC, we first need to transform our model to an unconstrained space using TransformVariables.jl and wrap it in a type that implements the LogDensityProblems interface (see DynamicHMC's worked example):","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"using DynamicHMC, ForwardDiff, LogDensityProblems, LogDensityProblemsAD, TransformVariables\nusing TransformedLogDensities: TransformedLogDensity\n\ntransform = as((σ=asℝ₊, α=asℝ, β=as(Array, J)))\nP = TransformedLogDensity(transform, model)\n∇P = ADgradient(:ForwardDiff, P)","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"Pathfinder can take any object that implements this interface.","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"result_pf = pathfinder(∇P)","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"init_params = result_pf.draws[:, 1]","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"inv_metric = result_pf.fit_distribution.Σ","category":"page"},{"location":"examples/initializing-hmc/#Initializing-from-Pathfinder's-draws","page":"Initializing HMC","title":"Initializing from Pathfinder's draws","text":"","category":"section"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"Here we just need to pass one of the draws as the initial point q:","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"result_dhmc1 = mcmc_with_warmup(\n    Random.GLOBAL_RNG,\n    ∇P,\n    ndraws;\n    initialization=(; q=init_params),\n    reporter=NoProgressReport(),\n)","category":"page"},{"location":"examples/initializing-hmc/#Initializing-metric-adaptation-from-Pathfinder's-estimate","page":"Initializing HMC","title":"Initializing metric adaptation from Pathfinder's estimate","text":"","category":"section"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"To start with Pathfinder's inverse metric estimate, we just need to initialize a DynamicHMC.GaussianKineticEnergy object with it as input: ","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"result_dhmc2 = mcmc_with_warmup(\n    Random.GLOBAL_RNG,\n    ∇P,\n    ndraws;\n    initialization=(; q=init_params, κ=GaussianKineticEnergy(inv_metric)),\n    warmup_stages=default_warmup_stages(; M=Symmetric),\n    reporter=NoProgressReport(),\n)","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"We also specified that DynamicHMC should tune a dense Symmetric matrix. However, we could also have requested a Diagonal metric.","category":"page"},{"location":"examples/initializing-hmc/#Use-Pathfinder's-metric-estimate-for-sampling","page":"Initializing HMC","title":"Use Pathfinder's metric estimate for sampling","text":"","category":"section"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"To turn off metric adaptation entirely and use Pathfinder's estimate, we just set the number and size of the metric adaptation windows to 0.","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"result_dhmc3 = mcmc_with_warmup(\n    Random.GLOBAL_RNG,\n    ∇P,\n    ndraws;\n    initialization=(; q=init_params, κ=GaussianKineticEnergy(inv_metric)),\n    warmup_stages=default_warmup_stages(; middle_steps=0, doubling_stages=0),\n    reporter=NoProgressReport(),\n)","category":"page"},{"location":"examples/initializing-hmc/#AdvancedHMC.jl","page":"Initializing HMC","title":"AdvancedHMC.jl","text":"","category":"section"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"Similar to Pathfinder and DynamicHMC, AdvancedHMC can also work with a LogDensityProblem:","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"using AdvancedHMC\n\nnadapts = 500;\nnothing # hide","category":"page"},{"location":"examples/initializing-hmc/#Initializing-from-Pathfinder's-draws-2","page":"Initializing HMC","title":"Initializing from Pathfinder's draws","text":"","category":"section"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"metric = DiagEuclideanMetric(dim)\nhamiltonian = Hamiltonian(metric, ∇P)\nϵ = find_good_stepsize(hamiltonian, init_params)\nintegrator = Leapfrog(ϵ)\nkernel = HMCKernel(Trajectory{MultinomialTS}(integrator, GeneralisedNoUTurn()))\nadaptor = StepSizeAdaptor(0.8, integrator)\nsamples_ahmc1, stats_ahmc1 = sample(\n    hamiltonian,\n    kernel,\n    init_params,\n    ndraws + nadapts,\n    adaptor,\n    nadapts;\n    drop_warmup=true,\n    progress=false,\n)","category":"page"},{"location":"examples/initializing-hmc/#Initializing-metric-adaptation-from-Pathfinder's-estimate-2","page":"Initializing HMC","title":"Initializing metric adaptation from Pathfinder's estimate","text":"","category":"section"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"We can't start with Pathfinder's inverse metric estimate directly. Instead we need to first extract its diagonal for a DiagonalEuclideanMetric or make it dense for a DenseEuclideanMetric:","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"metric = DenseEuclideanMetric(Matrix(inv_metric))\nhamiltonian = Hamiltonian(metric, ∇P)\nϵ = find_good_stepsize(hamiltonian, init_params)\nintegrator = Leapfrog(ϵ)\nkernel = HMCKernel(Trajectory{MultinomialTS}(integrator, GeneralisedNoUTurn()))\nadaptor = StepSizeAdaptor(0.8, integrator)\nsamples_ahmc2, stats_ahmc2 = sample(\n    hamiltonian,\n    kernel,\n    init_params,\n    ndraws + nadapts,\n    adaptor,\n    nadapts;\n    drop_warmup=true,\n    progress=false,\n)","category":"page"},{"location":"examples/initializing-hmc/#Use-Pathfinder's-metric-estimate-for-sampling-2","page":"Initializing HMC","title":"Use Pathfinder's metric estimate for sampling","text":"","category":"section"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"To use Pathfinder's metric with no metric adaptation, we need to use Pathfinder's own Pathfinder.RankUpdateEuclideanMetric type, which just wraps our inverse metric estimate for use with AdvancedHMC:","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"nadapts = 75\nmetric = Pathfinder.RankUpdateEuclideanMetric(inv_metric)\nhamiltonian = Hamiltonian(metric, ∇P)\nϵ = find_good_stepsize(hamiltonian, init_params)\nintegrator = Leapfrog(ϵ)\nkernel = HMCKernel(Trajectory{MultinomialTS}(integrator, GeneralisedNoUTurn()))\nadaptor = StepSizeAdaptor(0.8, integrator)\nsamples_ahmc3, stats_ahmc3 = sample(\n    hamiltonian,\n    kernel,\n    init_params,\n    ndraws + nadapts,\n    adaptor,\n    nadapts;\n    drop_warmup=true,\n    progress=false,\n)","category":"page"}]
}
