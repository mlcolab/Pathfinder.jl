var documenterSearchIndex = {"docs":
[{"location":"lib/internals/#Internals","page":"Internals","title":"Internals","text":"","category":"section"},{"location":"lib/internals/","page":"Internals","title":"Internals","text":"Documentation for Pathfinder.jl's internal functions.","category":"page"},{"location":"lib/internals/","page":"Internals","title":"Internals","text":"See the Public Documentation section for documentation of the public interface.","category":"page"},{"location":"lib/internals/#Index","page":"Internals","title":"Index","text":"","category":"section"},{"location":"lib/internals/","page":"Internals","title":"Internals","text":"Pages = [\"internals.md\"]","category":"page"},{"location":"lib/internals/#Internal-Interface","page":"Internals","title":"Internal Interface","text":"","category":"section"},{"location":"lib/internals/","page":"Internals","title":"Internals","text":"Modules = [Pathfinder]\nPublic = false\nPrivate = true","category":"page"},{"location":"lib/internals/#Pathfinder.RankUpdateEuclideanMetric","page":"Internals","title":"Pathfinder.RankUpdateEuclideanMetric","text":"RankUpdateEuclideanMetric{T,M} <: AdvancedHMC.AbstractMetric\n\nA Gaussian Euclidean metric whose inverse is constructed by rank-updates.\n\nConstructors\n\nRankUpdateEuclideanMetric(n::Int)\nRankUpdateEuclideanMetric(M⁻¹::Pathfinder.WoodburyPDMat)\n\nConstruct a Gaussian Euclidean metric of size (n, n) with inverse of M⁻¹.\n\nExample\n\njulia> using LinearAlgebra, Pathfinder, AdvancedHMC\n\njulia> Pathfinder.RankUpdateEuclideanMetric(3)\nRankUpdateEuclideanMetric(diag=[1.0, 1.0, 1.0])\n\njulia> W = Pathfinder.WoodburyPDMat(Diagonal([0.1, 0.2]), [0.7 0.2]', Diagonal([0.3]))\n2×2 Pathfinder.WoodburyPDMat{Float64, Diagonal{Float64, Vector{Float64}}, Adjoint{Float64, Matrix{Float64}}, Diagonal{Float64, Vector{Float64}}, Diagonal{Float64, Vector{Float64}}, QRCompactWYQ{Float64, Matrix{Float64}, Matrix{Float64}}, UpperTriangular{Float64, Matrix{Float64}}}:\n 0.247  0.042\n 0.042  0.212\n\njulia> Pathfinder.RankUpdateEuclideanMetric(W)\nRankUpdateEuclideanMetric(diag=[0.247, 0.21200000000000002])\n\n\n\n\n\n","category":"type"},{"location":"lib/internals/#Pathfinder.UniformSampler","page":"Internals","title":"Pathfinder.UniformSampler","text":"UniformSampler(scale::Real)\n\nSampler that in-place modifies an array to be IID uniformly distributed on [-scale, scale]\n\n\n\n\n\n","category":"type"},{"location":"lib/internals/#Pathfinder.WoodburyPDMat","page":"Internals","title":"Pathfinder.WoodburyPDMat","text":"WoodburyPDMat <: PDMats.AbstractPDMat\n\nLazily represents a real positive definite (PD) matrix as an update to a full-rank PD matrix.\n\nWoodburyPDMat(A, B, D)\n\nConstructs the n times n PD matrix\n\nW = A + B D B^mathrmT\n\nwhere A is an n times n full rank positive definite matrix, D is an m times m symmetric matrix, and B is an n times m matrix. Note that B and D must be chosen such that W is positive definite; this is only implicitly checked.\n\nOverloads for WoodburyPDMat make extensive use of the following decomposition. Let L_A L_A^mathrmT = A be the Cholesky decomposition of A, and let Q R = L_A^-1 B be a thin QR decomposition. Define C = I + RDR^mathrmT, with the Cholesky decomposition L_C L_C^mathrmT = C. Then, W = T T^mathrmT, where\n\nT = L_A Q beginpmatrix L_C  0  0  I endpmatrix\n\nThe positive definite requirement is equivalent to the requirement that both A and C are positive definite.\n\nFor a derivation of this decomposition for the special case of diagonal A, see appendix A of [Zhang2021].\n\n[Zhang2021]: Lu Zhang, Bob Carpenter, Andrew Gelman, Aki Vehtari (2021).           Pathfinder: Parallel quasi-Newton variational inference.           arXiv: 2108.03782 [stat.ML]\n\n\n\n\n\n","category":"type"},{"location":"lib/internals/#Pathfinder.fit_mvnormals-Tuple{Any, Any}","page":"Internals","title":"Pathfinder.fit_mvnormals","text":"fit_mvnormals(θs, ∇logpθs; history_length=5)\n\nFit a multivariate-normal distribution to each point on the trajectory θs.\n\nGiven θs with gradients ∇logpθs, construct LBFGS inverse Hessian approximations with the provided history_length. The inverse Hessians approximate a covariance. The covariances and corresponding means that define multivariate normal approximations per point are returned.\n\n\n\n\n\n","category":"method"},{"location":"lib/internals/#Pathfinder.flattened_varnames_list-Tuple{DynamicPPL.Model}","page":"Internals","title":"Pathfinder.flattened_varnames_list","text":"flattened_varnames_list(model::DynamicPPL.Model) -> Vector{Symbol}\n\nGet a vector of varnames as Symbols with one-to-one correspondence to the flattened parameter vector.\n\njulia> @model function demo()\n           s ~ Dirac(1)\n           x = Matrix{Float64}(undef, 2, 4)\n           x[1, 1] ~ Dirac(2)\n           x[2, 1] ~ Dirac(3)\n           x[3] ~ Dirac(4)\n           y ~ Dirac(5)\n           x[4] ~ Dirac(6)\n           x[:, 3] ~ arraydist([Dirac(7), Dirac(8)])\n           x[[2, 1], 4] ~ arraydist([Dirac(9), Dirac(10)])\n           return s, x, y\n       end\ndemo (generic function with 2 methods)\n\njulia> flattened_varnames_list(demo())\n10-element Vector{Symbol}:\n :s\n Symbol(\"x[1,1]\")\n Symbol(\"x[2,1]\")\n Symbol(\"x[3]\")\n Symbol(\"x[4]\")\n Symbol(\"x[:,3][1]\")\n Symbol(\"x[:,3][2]\")\n Symbol(\"x[[2, 1],4][1]\")\n Symbol(\"x[[2, 1],4][2]\")\n :y\n\n\n\n\n\n","category":"method"},{"location":"lib/internals/#Pathfinder.lbfgs_inverse_hessian-Tuple{LinearAlgebra.Diagonal, Any, Any, Any, Any}","page":"Internals","title":"Pathfinder.lbfgs_inverse_hessian","text":"lbfgs_inverse_hessian(H₀, S₀, Y₀, history_ind, history_length) -> WoodburyPDMat\n\nCompute approximate inverse Hessian initialized from H₀ from history stored in S₀ and Y₀.\n\nhistory_ind indicates the column in S₀ and Y₀ that was most recently added to the history, while history_length indicates the number of first columns in S₀ and Y₀ currently being used for storing history. S = S₀[:, history_ind+1:history_length; 1:history_ind] reorders the columns of ₀ so that the oldest is first and newest is last.\n\nFrom Theorem 2.2 of [Byrd1994], the expression for the inverse Hessian H is\n\nbeginalign\nB = beginpmatrixH_0 Y  Sendpmatrix\nR = operatornametriu(S^mathrmT Y)\nE = I circ R\nD = beginpmatrix\n    0  -R^-1\n    -R^-mathrmT  R^mathrm-T (E + Y^mathrmT H_0 Y ) R^mathrm-1\nendpmatrix\nH = H_0 + B D B^mathrmT\nendalign\n\n[Byrd1994]: Byrd, R.H., Nocedal, J. & Schnabel, R.B.          Representations of quasi-Newton matrices and their use in limited memory methods.          Mathematical Programming 63, 129–156 (1994).          doi: 10.1007/BF01582063\n\n\n\n\n\n","category":"method"},{"location":"lib/internals/#Pathfinder.lbfgs_inverse_hessians-Tuple{Any, Any}","page":"Internals","title":"Pathfinder.lbfgs_inverse_hessians","text":"lbfgs_inverse_hessians(θs, ∇logpθs; Hinit=gilbert_init, history_length=5, ϵ=1e-12) -> Vector{WoodburyPDMat}\n\nFrom an L-BFGS trajectory and gradients, compute the inverse Hessian approximations at each point.\n\nGiven positions θs with gradients ∇logpθs, construct LBFGS inverse Hessian approximations with the provided history_length.\n\n\n\n\n\n","category":"method"},{"location":"lib/internals/#Pathfinder.resample-NTuple{4, Any}","page":"Internals","title":"Pathfinder.resample","text":"resample(rng, x, log_weights, ndraws) -> (draws, psis_result)\nresample(rng, x, ndraws) -> draws\n\nDraw ndraws samples from x, with replacement.\n\nIf log_weights is provided, perform Pareto smoothed importance resampling.\n\n\n\n\n\n","category":"method"},{"location":"lib/internals/#Pathfinder.varnames_to_ranges","page":"Internals","title":"Pathfinder.varnames_to_ranges","text":"varnames_to_ranges(model::DynamicPPL.Model)\nvarnames_to_ranges(model::DynamicPPL.VarInfo)\nvarnames_to_ranges(model::DynamicPPL.Metadata)\n\nGet Dict mapping variable names in model to their ranges in a corresponding parameter vector.\n\nExamples\n\njulia> @model function demo()\n           s ~ Dirac(1)\n           x = Matrix{Float64}(undef, 2, 4)\n           x[1, 1] ~ Dirac(2)\n           x[2, 1] ~ Dirac(3)\n           x[3] ~ Dirac(4)\n           y ~ Dirac(5)\n           x[4] ~ Dirac(6)\n           x[:, 3] ~ arraydist([Dirac(7), Dirac(8)])\n           x[[2, 1], 4] ~ arraydist([Dirac(9), Dirac(10)])\n           return s, x, y\n       end\ndemo (generic function with 2 methods)\n\njulia> demo()()\n(1, Any[2.0 4.0 7 10; 3.0 6.0 8 9], 5)\n\njulia> varnames_to_ranges(demo())\nDict{AbstractPPL.VarName, UnitRange{Int64}} with 8 entries:\n  s           => 1:1\n  x[4]        => 5:5\n  x[:,3]      => 6:7\n  x[1,1]      => 2:2\n  x[2,1]      => 3:3\n  x[[2, 1],4] => 8:9\n  x[3]        => 4:4\n  y           => 10:10\n\n\n\n\n\n","category":"function"},{"location":"examples/quickstart/#Quickstart","page":"Quickstart","title":"Quickstart","text":"","category":"section"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"This page introduces basic Pathfinder usage with examples.","category":"page"},{"location":"examples/quickstart/#A-5-dimensional-multivariate-normal","page":"Quickstart","title":"A 5-dimensional multivariate normal","text":"","category":"section"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"For a simple example, we'll run Pathfinder on a multivariate normal distribution with a dense covariance matrix.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"using LinearAlgebra, Pathfinder, Printf, StatsPlots, Random\nRandom.seed!(42)\n\nΣ = [\n    2.71   0.5    0.19   0.07   1.04\n    0.5    1.11  -0.08  -0.17  -0.08\n    0.19  -0.08   0.26   0.07  -0.7\n    0.07  -0.17   0.07   0.11  -0.21\n    1.04  -0.08  -0.7   -0.21   8.65\n]\nμ = [-0.55, 0.49, -0.76, 0.25, 0.94]\nP = inv(Symmetric(Σ))\ndim = length(μ)\ninit_scale=4\n\nfunction logp_mvnormal(x)\n    z = x - μ\n    return -dot(z, P, z) / 2\nend\nnothing # hide","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"Now we run pathfinder.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"result = pathfinder(logp_mvnormal; dim, init_scale)","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"result is a PathfinderResult. See its docstring for a description of its fields.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"The L-BFGS optimizer constructs an approximation to the inverse Hessian of the negative log density using the limited history of previous points and gradients. For each iteration, Pathfinder uses this estimate as an approximation to the covariance matrix of a multivariate normal that approximates the target distribution. The distribution that maximizes the evidence lower bound (ELBO) is stored in result.fit_distribution. Its mean and covariance are quite close to our target distribution's.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"result.fit_distribution.μ","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"result.fit_distribution.Σ","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"result.draws is a Matrix whose columns are the requested draws from result.fit_distribution:","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"result.draws","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"We can visualize Pathfinder's sequence of multivariate-normal approximations with the following function:","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"function plot_pathfinder_trace(\n    result, logp_marginal, xrange, yrange, maxiters;\n    show_elbo=false, flipxy=false, kwargs...,\n)\n    iterations = min(length(result.optim_trace) - 1, maxiters)\n    trace_points = result.optim_trace.points\n    trace_dists = result.fit_distributions\n    anim = @animate for i in 1:iterations\n        contour(xrange, yrange, exp ∘ logp_marginal ∘ Base.vect; label=\"\")\n        trace = trace_points[1:(i + 1)]\n        dist = trace_dists[i + 1]\n        plot!(\n            first.(trace), last.(trace);\n            seriestype=:scatterpath, color=:black, msw=0, label=\"trace\",\n        )\n        covellipse!(\n            dist.μ[1:2], dist.Σ[1:2, 1:2];\n            n_std=2.45, alpha=0.7, color=1, linecolor=1, label=\"MvNormal 95% ellipsoid\",\n        )\n        title = \"Iteration $i\"\n        if show_elbo\n            est = result.elbo_estimates[i]\n            title *= \"  ELBO estimate: \" * @sprintf(\"%.1f\", est.value)\n        end\n        plot!(; xlims=extrema(xrange), ylims=extrema(yrange), title, kwargs...)\n    end\n    return anim\nend;\nnothing #hide","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"xrange = -5:0.1:5\nyrange = -5:0.1:5\n\nμ_marginal = μ[1:2]\nP_marginal = inv(Σ[1:2,1:2])\nlogp_mvnormal_marginal(x) = -dot(x - μ_marginal, P_marginal, x - μ_marginal) / 2\n\nanim = plot_pathfinder_trace(\n    result, logp_mvnormal_marginal, xrange, yrange, 20;\n    xlabel=\"x₁\", ylabel=\"x₂\",\n)\ngif(anim; fps=5)","category":"page"},{"location":"examples/quickstart/#A-banana-shaped-distribution","page":"Quickstart","title":"A banana-shaped distribution","text":"","category":"section"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"Now we will run Pathfinder on the following banana-shaped distribution with density","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"pi(x_1 x_2) = e^-x_1^2  2 e^-5 (x_2 - x_1^2)^2  2","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"First we define the distribution:","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"Random.seed!(23)\n\nlogp_banana(x) = -(x[1]^2 + 5(x[2] - x[1]^2)^2) / 2\ndim = 2\ninit_scale = 10\n\nnothing # hide","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"and then visualise it:","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"xrange = -3.5:0.05:3.5\nyrange = -3:0.05:7\ncontour(xrange, yrange, exp ∘ logp_banana ∘ Base.vect; xlabel=\"x₁\", ylabel=\"x₂\")","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"Now we run pathfinder.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"result = pathfinder(logp_banana; dim, init_scale)","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"As before we can visualise each iteration of the algorithm.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"anim = plot_pathfinder_trace(\n    result, logp_banana, xrange, yrange, 20;\n    xlabel=\"x₁\", ylabel=\"x₂\",\n)\ngif(anim; fps=5)","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"Since the distribution is far from normal, Pathfinder is unable to fit the distribution well. Especially for such complicated target distributions, it's always a good idea to run multipathfinder, which runs single-path Pathfinder multiple times.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"ndraws = 1_000\nresult = multipathfinder(logp_banana, ndraws; nruns=20, dim, init_scale)","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"result is a MultiPathfinderResult. See its docstring for a description of its fields.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"result.fit_distribution is a uniformly-weighted Distributions.MixtureModel. Each component is the result of a single Pathfinder run. It's possible that some runs fit the target distribution much better than others, so instead of just drawing samples from result.fit_distribution, multipathfinder draws many samples from each component and then uses Pareto-smoothed importance resampling (using PSIS.jl) from these draws to better target logp_banana.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"The Pareto shape diagnostic informs us on the quality of these draws. Here the Pareto shape k diagnostic is bad (k  07), which tells us that these draws are unsuitable for computing posterior estimates, so we should definitely run MCMC to get better draws. Still, visualizing the draws can still be useful.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"x₁_approx = result.draws[1, :]\nx₂_approx = result.draws[2, :]\n\ncontour(xrange, yrange, exp ∘ logp_banana ∘ Base.vect)\nscatter!(x₁_approx, x₂_approx; msw=0, ms=2, alpha=0.5, color=1)\nplot!(xlims=extrema(xrange), ylims=extrema(yrange), xlabel=\"x₁\", ylabel=\"x₂\", legend=false)","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"While the draws do a poor job of covering the tails of the distribution, they are still useful for identifying the nonlinear correlation between these two parameters.","category":"page"},{"location":"examples/quickstart/#A-100-dimensional-funnel","page":"Quickstart","title":"A 100-dimensional funnel","text":"","category":"section"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"As we have seen above, running multi-path Pathfinder is much more useful for target distributions that are far from normal. One particularly difficult distribution to sample is Neal's funnel:","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"beginaligned\ntau sim mathrmNormal(mu=0 sigma=3)\nbeta_i sim mathrmNormal(mu=0 sigma=e^tau2)\nendaligned","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"Such funnel geometries appear in other models (e.g. many hierarchical models) and typically frustrate MCMC sampling. Multi-path Pathfinder can't sample the funnel well, but it can quickly give us draws that can help us diagnose that we have a funnel.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"In this example, we draw from a 100-dimensional funnel and visualize 2 dimensions.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"Random.seed!(68)\n\nfunction logp_funnel(x)\n    n = length(x)\n    τ = x[1]\n    β = view(x, 2:n)\n    return ((τ / 3)^2 + (n - 1) * τ + sum(b -> abs2(b * exp(-τ / 2)), β)) / -2\nend\n\ndim = 100\ninit_scale = 10\nnothing # hide","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"First, let's fit this posterior with single-path Pathfinder.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"result_single = pathfinder(logp_funnel; dim, init_scale)","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"Let's visualize this sequence of multivariate normals for the first two dimensions.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"β₁_range = -5:0.01:5\nτ_range = -15:0.01:5\n\nanim = plot_pathfinder_trace(\n    result_single, logp_funnel, τ_range, β₁_range, 15;\n    show_elbo=true, xlabel=\"τ\", ylabel=\"β₁\",\n)\ngif(anim; fps=2)","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"For this challenging posterior, we can again see that most of the approximations are not great, because this distribution is not normal. Also, this distribution has a pole instead of a mode, so there is no MAP estimate, and no Laplace approximation exists. As optimization proceeds, the approximation goes from very bad to less bad and finally extremely bad. The ELBO-maximizing distribution is at the neck of the funnel, which would be a good location to initialize MCMC.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"Now we run multipathfinder.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"ndraws = 1_000\nresult = multipathfinder(logp_funnel, ndraws; nruns=20, dim, init_scale)","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"Again, the poor Pareto shape diagnostic indicates we should run MCMC to get draws suitable for computing posterior estimates.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"We can see that the bulk of Pathfinder's draws come from the neck of the funnel, where the fit from the single path we examined was located.","category":"page"},{"location":"examples/quickstart/","page":"Quickstart","title":"Quickstart","text":"τ_approx = result.draws[1, :]\nβ₁_approx = result.draws[2, :]\n\ncontour(τ_range, β₁_range, exp ∘ logp_funnel ∘ Base.vect)\nscatter!(τ_approx, β₁_approx; msw=0, ms=2, alpha=0.5, color=1)\nplot!(; xlims=extrema(τ_range), ylims=extrema(β₁_range), xlabel=\"τ\", ylabel=\"β₁\", legend=false)","category":"page"},{"location":"examples/turing/#Running-Pathfinder-on-Turing.jl-models","page":"Turing usage","title":"Running Pathfinder on Turing.jl models","text":"","category":"section"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"This tutorial demonstrates how Turing can be used with Pathfinder.","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"We'll demonstrate with a regression example.","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"using AdvancedHMC, LinearAlgebra, Pathfinder, Random, Turing\nRandom.seed!(39)\n\n@model function regress(x, y)\n    α ~ Normal()\n    β ~ Normal()\n    σ ~ truncated(Normal(); lower=0)\n    y .~ Normal.(α .+ β .* x, σ)\nend\nx = 0:0.1:10\ny = @. 2x + 1.5 + randn() * 0.2\nnothing # hide","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"model = regress(collect(x), y)","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"The first way we can use Turing with Pathfinder is via its mode estimation functionality. We can use Turing.optim_function to generate a SciMLBase.OptimizationFunction, which pathfinder and multipathfinder can take as inputs.","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"fun = optim_function(model, MAP(); constrained=false)","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"dim = length(fun.init())\npathfinder(fun.func; dim)","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"multipathfinder(fun.func, 1_000; dim, nruns=8)","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"However, for convenience, pathfinder and multipathfinder can take Turing models as inputs and produce MCMCChains.Chains objects as outputs.","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"result_single = pathfinder(model; ndraws=1_000)","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"result_multi = multipathfinder(model, 1_000; nruns=8)","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"Here, the Pareto shape diagnostic indicates that it is likely safe to use these draws to compute posterior estimates.","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"When passed a Model, Pathfinder also gives access to the posterior draws in a familiar MCMCChains.Chains object.","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"result_multi.draws_transformed","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"We can also use these posterior draws to initialize MCMC sampling.","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"init_params = collect.(eachrow(result_multi.draws_transformed.value[1:4, :, 1]))","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"chns = sample(model, Turing.NUTS(), MCMCThreads(), 1_000, 4; init_params, progress=false)","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"To use Pathfinder's estimate of the metric and skip warm-up, at the moment one needs to use AdvancedHMC directly.","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"ℓπ(x) = -fun.func.f(x, nothing)\nfunction ∂ℓπ∂θ(x)\n    g = similar(x)\n    fun.func.grad(g, x, nothing)\n    rmul!(g, -1)\n    return ℓπ(x), g\nend\n\nndraws = 1_000\nnadapts = 50\ninv_metric = result_multi.pathfinder_results[1].fit_distribution.Σ\nmetric = Pathfinder.RankUpdateEuclideanMetric(inv_metric)\nhamiltonian = Hamiltonian(metric, ℓπ, ∂ℓπ∂θ)\nϵ = find_good_stepsize(hamiltonian, init_params[1])\nintegrator = Leapfrog(ϵ)\nproposal = AdvancedHMC.NUTS{MultinomialTS,GeneralisedNoUTurn}(integrator)\nadaptor = StepSizeAdaptor(0.8, integrator)\nsamples, stats = sample(\n    hamiltonian,\n    proposal,\n    init_params[1],\n    ndraws + nadapts,\n    adaptor,\n    nadapts;\n    drop_warmup=true,\n    progress=false,\n)","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"Now we pack the samples into an MCMCChains.Chains:","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"samples_transformed = reduce(vcat, fun.transform.(samples)')\nvarnames = Pathfinder.flattened_varnames_list(model)\nchns = MCMCChains.Chains(samples_transformed, varnames)","category":"page"},{"location":"examples/turing/","page":"Turing usage","title":"Turing usage","text":"See Initializing HMC with Pathfinder for further examples.","category":"page"},{"location":"lib/public/#Public-Documentation","page":"Public","title":"Public Documentation","text":"","category":"section"},{"location":"lib/public/","page":"Public","title":"Public","text":"Documentation for Pathfinder.jl's public interface.","category":"page"},{"location":"lib/public/","page":"Public","title":"Public","text":"See the Internals section for documentation of internal functions.","category":"page"},{"location":"lib/public/#Index","page":"Public","title":"Index","text":"","category":"section"},{"location":"lib/public/","page":"Public","title":"Public","text":"Pages = [\"public.md\"]","category":"page"},{"location":"lib/public/#Public-Interface","page":"Public","title":"Public Interface","text":"","category":"section"},{"location":"lib/public/","page":"Public","title":"Public","text":"Modules = [Pathfinder]\nPages = [\"singlepath.jl\"]\nOrder = [:function, :type]\nPublic = true\nPrivate = false","category":"page"},{"location":"lib/public/#Pathfinder.pathfinder","page":"Public","title":"Pathfinder.pathfinder","text":"pathfinder(logp; kwargs...)\npathfinder(logp, ∇logp; kwargs...)\npathfinder(fun::SciMLBase::OptimizationFunction; kwargs...)\npathfinder(prob::SciMLBase::OptimizationProblem; kwargs...)\n\nFind the best multivariate normal approximation encountered while maximizing logp.\n\nFrom an optimization trajectory, Pathfinder constructs a sequence of (multivariate normal) approximations to the distribution specified by logp. The approximation that maximizes the evidence lower bound (ELBO), or equivalently, minimizes the KL divergence between the approximation and the true distribution, is returned.\n\nThe covariance of the multivariate normal distribution is an inverse Hessian approximation constructed using at most the previous history_length steps.\n\nArguments\n\nlogp: a callable that computes the log-density of the target distribution.\n∇logp: a callable that computes the gradient of logp. If not provided, logp is   automatically differentiated using the backend specified in ad_backend.\nfun::SciMLBase.OptimizationFunction: an optimization function that represents   -logp(x) with its gradient. It must have the necessary features (e.g. a Hessian   function) for the chosen optimization algorithm. For details, see   Optimization.jl: OptimizationFunction.\nprob::SciMLBase.OptimizationProblem: an optimization problem containing a function with   the same properties as fun, as well as an initial point, in which case init and   dim are ignored.\n\nKeywords\n\ndim::Int: dimension of the target distribution. If not provided, init or must be.   Ignored if init is provided.\ninit::AbstractVector{<:Real}: initial point of length dim from which to begin   optimization. If not provided, an initial point of type Vector{Float64} and length   dim is created and filled using init_sampler.\ninit_scale::Real: scale factor s such that the default init_sampler samples   entries uniformly in the range -s s\ninit_sampler: function with the signature (rng, x) -> x that modifies a vector of   length dims in-place to generate an initial point\nndraws_elbo::Int=5: Number of draws used to estimate the ELBO\nndraws::Int=ndraws_elbo: number of approximate draws to return\nad_backend=AD.ForwardDiffBackend(): AbstractDifferentiation.jl AD backend.\nrng::Random.AbstractRNG: The random number generator to be used for drawing samples\nexecutor::Transducers.Executor=Transducers.SequentialEx(): Transducers.jl executor that   determines if and how to perform ELBO computation in parallel. The default   (SequentialEx()) performs no parallelization. If rng is known to be thread-safe, and   the log-density function is known to have no internal state, then   Transducers.PreferParallel() may be used to parallelize log-density evaluation.   This is generally only faster for expensive log density functions.\nhistory_length::Int=6: Size of the history used to approximate the   inverse Hessian.\noptimizer: Optimizer to be used for constructing trajectory. Can be any optimizer   compatible with Optimization.jl, so long as it supports callbacks. Defaults to   Optim.LBFGS(; m=history_length, linesearch=LineSearches.MoreThuente()). See   the Optimization.jl documentation for details.\nntries::Int=1_000: Number of times to try the optimization, restarting if it fails. Before   every restart, a new initial point is drawn using init_sampler.\nfail_on_nonfinite::Bool=true: If true, optimization fails if the log-density is a   NaN or Inf or if the gradient is ever non-finite. If nretries > 0, then   optimization will be retried after reinitialization.\nkwargs... : Remaining keywords are forwarded to   Optimization.solve.\n\nReturns\n\nPathfinderResult\n\n\n\n\n\n","category":"function"},{"location":"lib/public/#Pathfinder.PathfinderResult","page":"Public","title":"Pathfinder.PathfinderResult","text":"PathfinderResult\n\nContainer for results of single-path Pathfinder.\n\nFields\n\ninput: User-provided input object, e.g. either logp, (logp, ∇logp), optim_fun,   optim_prob, or another object.\noptimizer: Optimizer used for maximizing the log-density\nrng: Pseudorandom number generator that was used for sampling\noptim_prob::SciMLBase.OptimizationProblem: Otimization problem used for   optimization\nlogp: Log-density function\nfit_distribution::Distributions.MvNormal: ELBO-maximizing multivariate normal   distribution\ndraws::AbstractMatrix{<:Real}: draws from multivariate normal with size (dim, ndraws)\nfit_distribution_transformed: fit_distribution transformed to the same space as the   user-supplied target distribution. This is only different from fit_distribution when   integrating with other packages, and its type depends on the type of input.\ndraws_transformed: draws transformed to be draws from fit_distribution_transformed.\nfit_iteration::Int: Iteration at which ELBO estimate was maximized\nnum_tries::Int: Number of tries until Pathfinder succeeded\noptim_solution::SciMLBase.OptimizationSolution: Solution object of optimization.\noptim_trace::Pathfinder.OptimizationTrace: container for optimization trace of points,   log-density, and gradient. The first point is the initial point.\nfit_distributions::AbstractVector{Distributions.MvNormal}: Multivariate normal   distributions for each point in optim_trace, where   fit_distributions[fit_iteration + 1] == fit_distribution\nelbo_estimates::AbstractVector{<:Pathfinder.ELBOEstimate}: ELBO estimates for all but   the first distribution in fit_distributions.\n\nReturns\n\nPathfinderResult\n\n\n\n\n\n","category":"type"},{"location":"lib/public/","page":"Public","title":"Public","text":"Modules = [Pathfinder]\nPages = [\"multipath.jl\"]\nOrder = [:function, :type]\nPublic = true\nPrivate = false","category":"page"},{"location":"lib/public/#Pathfinder.multipathfinder","page":"Public","title":"Pathfinder.multipathfinder","text":"multipathfinder(logp, ndraws; kwargs...)\nmultipathfinder(logp, ∇logp, ndraws; kwargs...)\nmultipathfinder(fun::SciMLBase.OptimizationFunction, ndraws; kwargs...)\n\nRun pathfinder multiple times to fit a multivariate normal mixture model.\n\nFor nruns=length(init), nruns parallel runs of pathfinder produce nruns multivariate normal approximations q_k = q(phi  mu_k Sigma_k) of the posterior. These are combined to a mixture model q with uniform weights.\n\nq is augmented with the component index to generate random samples, that is, elements (k phi) are drawn from the augmented mixture model\n\ntildeq(phi k  mu Sigma) = K^-1 q(phi  mu_k Sigma_k)\n\nwhere k is a component index, and K= nruns. These draws are then resampled with replacement. Discarding k from the samples would reproduce draws from q.\n\nIf importance=true, then Pareto smoothed importance resampling is used, so that the resulting draws better approximate draws from the target distribution p instead of q. This also prints a warning message if the importance weighted draws are unsuitable for approximating expectations with respect to p.\n\nArguments\n\nlogp: a callable that computes the log-density of the target distribution.\n∇logp: a callable that computes the gradient of logp. If not provided, logp is   automatically differentiated using the backend specified in ad_backend.\nfun::SciMLBase.OptimizationFunction: an optimization function that represents   -logp(x) with its gradient. It must have the necessary features (e.g. a Hessian   function) for the chosen optimization algorithm. For details, see   Optimization.jl: OptimizationFunction.\nndraws::Int: number of approximate draws to return\n\nKeywords\n\ninit: iterator of length nruns of initial points of length dim from which each   single-path Pathfinder run will begin. length(init) must be implemented. If init is   not provided, dim and nruns must be.\nnruns::Int: number of runs of Pathfinder to perform. Ignored if init is provided.\nad_backend=AD.ForwardDiffBackend(): AbstractDifferentiation.jl AD backend used to   differentiate logp if ∇logp is not provided.\nndraws_per_run::Int: The number of draws to take for each component before resampling.   Defaults to a number such that ndraws_per_run * nruns > ndraws.\nimportance::Bool=true: Perform Pareto smoothed importance resampling of draws.\nrng::AbstractRNG=Random.GLOBAL_RNG: Pseudorandom number generator. It is recommended to   use a parallelization-friendly PRNG like the default PRNG on Julia 1.7 and up.\nexecutor::Transducers.Executor: Transducers.jl executor that determines if and how   to run the single-path runs in parallel. If rng is known to be thread-safe, the   default is Transducers.PreferParallel() (parallel executation, defaulting to   multi-threading). Otherwise, it is Transducers.SequentialEx() (no parallelization).\nexecutor_per_run::Transducers.Executor=Transducers.SequentialEx(): Transducers.jl   executor used within each run to parallelize PRNG calls. Defaults to no parallelization.   See pathfinder for a description.\nkwargs... : Remaining keywords are forwarded to pathfinder.\n\nReturns\n\nMultiPathfinderResult\n\n\n\n\n\n","category":"function"},{"location":"lib/public/#Pathfinder.MultiPathfinderResult","page":"Public","title":"Pathfinder.MultiPathfinderResult","text":"MultiPathfinderResult\n\nContainer for results of multi-path Pathfinder.\n\nFields\n\ninput: User-provided input object, e.g. either logp, (logp, ∇logp), optim_fun,   optim_prob, or another object.\noptimizer: Optimizer used for maximizing the log-density\nrng: Pseudorandom number generator that was used for sampling\noptim_prob::SciMLBase.OptimizationProblem: Otimization problem used for   optimization\nlogp: Log-density function\nfit_distribution::Distributions.MixtureModel: uniformly-weighted mixture of ELBO-   maximizing multivariate normal distributions from each run.\ndraws::AbstractMatrix{<:Real}: draws from fit_distribution with size (dim, ndraws),   potentially resampled using importance resampling to be closer to the target   distribution.\ndraw_component_ids::Vector{Int}: component id of each draw in draws.\nfit_distribution_transformed: fit_distribution transformed to the same space as the   user-supplied target distribution. This is only different from fit_distribution when   integrating with other packages, and its type depends on the type of input.\ndraws_transformed: draws transformed to be draws from fit_distribution_transformed.\npathfinder_results::Vector{<:PathfinderResult}: results of each single-path Pathfinder   run.\npsis_result::Union{Nothing,<:PSIS.PSISResult}: If importance resampling was used, the   result of Pareto-smoothed importance resampling. psis_result.pareto_shape also   diagnoses whether draws can be used to compute estimates from the target distribution.   See PSIS.PSISResult for   details\n\n\n\n\n\n","category":"type"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = Pathfinder","category":"page"},{"location":"#Pathfinder.jl:-Parallel-quasi-Newton-variational-inference","page":"Home","title":"Pathfinder.jl: Parallel quasi-Newton variational inference","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package implements Pathfinder, [Zhang2021] a variational method for initializing Markov chain Monte Carlo (MCMC) methods.","category":"page"},{"location":"#Single-path-Pathfinder","page":"Home","title":"Single-path Pathfinder","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Single-path Pathfinder (pathfinder) attempts to return draws in or near the typical set, usually with many fewer gradient evaluations. Pathfinder uses the limited-memory BFGS(L-BFGS) optimizer to construct a maximum a posteriori (MAP) estimate of a target posterior distribution p. It then uses the trace of the optimization to construct a sequence of multivariate normal approximations to the target distribution, returning the approximation that maximizes the evidence lower bound (ELBO) – equivalently, minimizes the Kullback-Leibler divergence from the target distribution – as well as draws from the distribution.","category":"page"},{"location":"#Multi-path-Pathfinder","page":"Home","title":"Multi-path Pathfinder","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Multi-path Pathfinder (multipathfinder) consists of running Pathfinder multiple times. It returns a uniformly-weighted mixture model of the multivariate normal approximations of the individual runs. It also uses importance resampling to return samples that better approximate the target distribution and assess the quality of the approximation.","category":"page"},{"location":"#Uses","page":"Home","title":"Uses","text":"","category":"section"},{"location":"#Using-the-Pathfinder-draws","page":"Home","title":"Using the Pathfinder draws","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"note: Folk theorem of statistical computing\nWhen you have computational problems, often there’s a problem with your model.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Visualizing posterior draws is a common way to diagnose problems with a model.  However, problematic models often tend to be slow to warm-up. Even if the draws returned by Pathfinder are only approximations to the posterior, they can sometimes still be used to diagnose basic issues such as highly correlated parameters, parameters with very different posterior variances, and multimodality.","category":"page"},{"location":"#Initializing-MCMC","page":"Home","title":"Initializing MCMC","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pathfinder can be used to initialize MCMC. This especially useful when sampling with Hamiltonian Monte Carlo. See Initializing HMC with Pathfinder for details.","category":"page"},{"location":"#Integration-with-the-Julia-ecosystem","page":"Home","title":"Integration with the Julia ecosystem","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pathfinder uses several packages for extended functionality:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Optimization.jl: This allows the L-BFGS optimizer to be replaced with any of the many Optimization-compatible optimizers and supports use of callbacks. Note that any changes made to Pathfinder using these features would be experimental.\nTransducers.jl: parallelization support\nDistributions.jl/PDMats.jl: fits can be used anywhere a Distribution can be used\nAbstractDifferentiation.jl: selecting the AD package used to differentiate the provided log-density function.\nProgressLogging.jl: In Pluto, Juno, and VSCode, nested progress bars are shown. In the REPL, use TerminalLoggers.jl to get progress bars.","category":"page"},{"location":"","page":"Home","title":"Home","text":"[Zhang2021]: Lu Zhang, Bob Carpenter, Andrew Gelman, Aki Vehtari (2021).           Pathfinder: Parallel quasi-Newton variational inference.           arXiv: 2108.03782 [stat.ML].           Code","category":"page"},{"location":"examples/initializing-hmc/#Initializing-HMC-with-Pathfinder","page":"Initializing HMC","title":"Initializing HMC with Pathfinder","text":"","category":"section"},{"location":"examples/initializing-hmc/#The-MCMC-warm-up-phase","page":"Initializing HMC","title":"The MCMC warm-up phase","text":"","category":"section"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"When using MCMC to draw samples from some target distribution, there is often a lengthy warm-up phase with 2 phases:","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"converge to the typical set (the region of the target distribution where the bulk of the probability mass is located)\nadapt any tunable parameters of the MCMC sampler (optional)","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"While (1) often happens fairly quickly, (2) usually requires a lengthy exploration of the typical set to iteratively adapt parameters suitable for further exploration. An example is the widely used windowed adaptation scheme of Hamiltonian Monte Carlo (HMC) in Stan, where a step size and positive definite metric (aka mass matrix) are adapted.[1] For posteriors with complex geometry, the adaptation phase can require many evaluations of the gradient of the log density function of the target distribution.","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"Pathfinder can be used to initialize MCMC, and in particular HMC, in 3 ways:","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"Initialize MCMC from one of Pathfinder's draws (replace phase 1 of the warm-up).\nInitialize an HMC metric adaptation from the inverse of the covariance of the multivariate normal approximation (replace the first window of phase 2 of the warm-up).\nUse the inverse of the covariance as the metric without further adaptation (replace phase 2 of the warm-up).","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"This tutorial demonstrates all three approaches with DynamicHMC.jl and AdvancedHMC.jl. Both of these packages have standalone implementations of adaptive HMC (aka NUTS) and can be used independently of any probabilistic programming language (PPL). Both the Turing and Soss PPLs have some DynamicHMC integration, while Turing also integrates with AdvancedHMC.","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"For demonstration purposes, we'll use the following dummy data:","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"using LinearAlgebra, Pathfinder, Random, StatsFuns, StatsPlots\n\nRandom.seed!(91)\n\nx = 0:0.01:1\ny = @. sin(10x) + randn() * 0.2 + x\n\nscatter(x, y; xlabel=\"x\", ylabel=\"y\", legend=false, msw=0, ms=2)","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"We'll fit this using a simple polynomial regression model:","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"beginaligned\nsigma sim textHalf-Normal(mu=0 sigma=1)\nalpha beta_j sim mathrmNormal(mu=0 sigma=1)\nhaty_i = alpha + sum_j=1^J x_i^j beta_j\ny_i sim mathrmNormal(mu=haty_i sigma=sigma)\nendaligned","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"We just need to implement the log-density function of the resulting posterior.","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"struct RegressionProblem{X,Z,Y}\n    x::X\n    J::Int\n    z::Z\n    y::Y\nend\nfunction RegressionProblem(x, J, y)\n    z = x .* (1:J)'\n    return RegressionProblem(x, J, z, y)\nend\n\nfunction (prob::RegressionProblem)(θ)\n    σ = θ.σ\n    α = θ.α\n    β = θ.β\n    z = prob.z\n    y = prob.y\n    lp = normlogpdf(σ) + logtwo\n    lp += normlogpdf(α)\n    lp += sum(normlogpdf, β)\n    y_hat = muladd(z, β, α)\n    lp += sum(eachindex(y_hat, y)) do i\n        return normlogpdf(y_hat[i], σ, y[i])\n    end\n    return lp\nend\n\nJ = 3\ndim = J + 2\nmodel = RegressionProblem(x, J, y)\nndraws = 1_000;\nnothing # hide","category":"page"},{"location":"examples/initializing-hmc/#DynamicHMC.jl","page":"Initializing HMC","title":"DynamicHMC.jl","text":"","category":"section"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"To use DynamicHMC, we first need to transform our model to an unconstrained space using TransformVariables.jl and wrap it in a type that implements the LogDensityProblems.jl interface:","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"using DynamicHMC, LogDensityProblems, TransformVariables\nusing TransformedLogDensities: TransformedLogDensity\n\ntransform = as((σ=asℝ₊, α=asℝ, β=as(Array, J)))\nP = TransformedLogDensity(transform, model)\n∇P = ADgradient(:ForwardDiff, P)","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"Pathfinder, on the other hand, expects a log-density function:","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"logp(x) = LogDensityProblems.logdensity(P, x)\n∇logp(x) = LogDensityProblems.logdensity_and_gradient(∇P, x)[2]\nresult_pf = pathfinder(logp, ∇logp; dim)","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"init_params = result_pf.draws[:, 1]","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"inv_metric = result_pf.fit_distribution.Σ","category":"page"},{"location":"examples/initializing-hmc/#Initializing-from-Pathfinder's-draws","page":"Initializing HMC","title":"Initializing from Pathfinder's draws","text":"","category":"section"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"Here we just need to pass one of the draws as the initial point q:","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"result_dhmc1 = mcmc_with_warmup(\n    Random.GLOBAL_RNG,\n    ∇P,\n    ndraws;\n    initialization=(; q=init_params),\n    reporter=NoProgressReport(),\n)","category":"page"},{"location":"examples/initializing-hmc/#Initializing-metric-adaptation-from-Pathfinder's-estimate","page":"Initializing HMC","title":"Initializing metric adaptation from Pathfinder's estimate","text":"","category":"section"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"To start with Pathfinder's inverse metric estimate, we just need to initialize a GaussianKineticEnergy object with it as input: ","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"result_dhmc2 = mcmc_with_warmup(\n    Random.GLOBAL_RNG,\n    ∇P,\n    ndraws;\n    initialization=(; q=init_params, κ=GaussianKineticEnergy(inv_metric)),\n    warmup_stages=default_warmup_stages(; M=Symmetric),\n    reporter=NoProgressReport(),\n)","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"We also specified that DynamicHMC should tune a dense Symmetric matrix. However, we could also have requested a Diagonal metric.","category":"page"},{"location":"examples/initializing-hmc/#Use-Pathfinder's-metric-estimate-for-sampling","page":"Initializing HMC","title":"Use Pathfinder's metric estimate for sampling","text":"","category":"section"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"To turn off metric adaptation entirely and use Pathfinder's estimate, we just set the number and size of the metric adaptation windows to 0.","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"result_dhmc3 = mcmc_with_warmup(\n    Random.GLOBAL_RNG,\n    ∇P,\n    ndraws;\n    initialization=(; q=init_params, κ=GaussianKineticEnergy(inv_metric)),\n    warmup_stages=default_warmup_stages(; middle_steps=0, doubling_stages=0),\n    reporter=NoProgressReport(),\n)","category":"page"},{"location":"examples/initializing-hmc/#AdvancedHMC.jl","page":"Initializing HMC","title":"AdvancedHMC.jl","text":"","category":"section"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"Similar to Pathfinder, AdvancedHMC works with an unconstrained log density function and its gradient. We'll just use the logp we already created above.","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"using AdvancedHMC, ForwardDiff\n\nnadapts = 500;\nnothing # hide","category":"page"},{"location":"examples/initializing-hmc/#Initializing-from-Pathfinder's-draws-2","page":"Initializing HMC","title":"Initializing from Pathfinder's draws","text":"","category":"section"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"metric = DiagEuclideanMetric(dim)\nhamiltonian = Hamiltonian(metric, logp, ForwardDiff)\nϵ = find_good_stepsize(hamiltonian, init_params)\nintegrator = Leapfrog(ϵ)\nproposal = NUTS{MultinomialTS,GeneralisedNoUTurn}(integrator)\nadaptor = StepSizeAdaptor(0.8, integrator)\nsamples_ahmc1, stats_ahmc1 = sample(\n    hamiltonian,\n    proposal,\n    init_params,\n    ndraws + nadapts,\n    adaptor,\n    nadapts;\n    drop_warmup=true,\n    progress=false,\n)","category":"page"},{"location":"examples/initializing-hmc/#Initializing-metric-adaptation-from-Pathfinder's-estimate-2","page":"Initializing HMC","title":"Initializing metric adaptation from Pathfinder's estimate","text":"","category":"section"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"We can't start with Pathfinder's inverse metric estimate directly. Instead we need to first extract its diagonal for a DiagonalEuclideanMetric or make it dense for a DenseEuclideanMetric:","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"metric = DenseEuclideanMetric(Matrix(inv_metric))\nhamiltonian = Hamiltonian(metric, logp, ForwardDiff)\nϵ = find_good_stepsize(hamiltonian, init_params)\nintegrator = Leapfrog(ϵ)\nproposal = NUTS{MultinomialTS,GeneralisedNoUTurn}(integrator)\nadaptor = StepSizeAdaptor(0.8, integrator)\nsamples_ahmc2, stats_ahmc2 = sample(\n    hamiltonian,\n    proposal,\n    init_params,\n    ndraws + nadapts,\n    adaptor,\n    nadapts;\n    drop_warmup=true,\n    progress=false,\n)","category":"page"},{"location":"examples/initializing-hmc/#Use-Pathfinder's-metric-estimate-for-sampling-2","page":"Initializing HMC","title":"Use Pathfinder's metric estimate for sampling","text":"","category":"section"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"To use Pathfinder's metric with no metric adaptation, we need to use Pathfinder's own RankUpdateEuclideanMetric type, which just wraps our inverse metric estimate for use with AdvancedHMC:","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"nadapts = 75\nmetric = Pathfinder.RankUpdateEuclideanMetric(inv_metric)\nhamiltonian = Hamiltonian(metric, logp, ForwardDiff)\nϵ = find_good_stepsize(hamiltonian, init_params)\nintegrator = Leapfrog(ϵ)\nproposal = NUTS{MultinomialTS,GeneralisedNoUTurn}(integrator)\nadaptor = StepSizeAdaptor(0.8, integrator)\nsamples_ahmc3, stats_ahmc3 = sample(\n    hamiltonian,\n    proposal,\n    init_params,\n    ndraws + nadapts,\n    adaptor,\n    nadapts;\n    drop_warmup=true,\n    progress=false,\n)","category":"page"},{"location":"examples/initializing-hmc/","page":"Initializing HMC","title":"Initializing HMC","text":"[1]: https://mc-stan.org/docs/reference-manual/hmc-algorithm-parameters.html","category":"page"}]
}
